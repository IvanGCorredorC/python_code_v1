{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Análisis de sentimiento\n","\n","Vamos a trabajar con el dataset de TensorFlow `imdb_reviews`, un conjunto de datos ampliamente utilizado para tareas de procesamiento de lenguaje natural, especialmente para la clasificación de sentimientos.\n","\n","1. **Contenido del conjunto de datos**:\n","   - **Comentarios**: El dataset contiene 50,000 comentarios de películas extraídos de la base de datos de Internet Movie Database (IMDb).\n","   - **Texto sin procesar**: Los comentarios están en formato de texto sin procesar.\n","   - **Etiquetas**: Cada comentario está etiquetado con un sentimiento: `positivo: 1` o `negativo: 0`.\n","\n","2. **División del conjunto de datos**:\n","   - **Entrenamiento**: 25,000 comentarios.\n","   - **Prueba**: 25,000 comentarios.\n","   - Las divisiones están balanceadas, es decir, hay un número igual de comentarios positivos y negativos en cada conjunto.\n","\n","3. **Objetivo**:\n","   - **Clasificación de sentimientos**: Vamos a utilizar este conjunto de datos para entrenar y evaluar modelos de clasificación de texto, con el objetivo de determinar si un comentario tiene un sentimiento positivo o negativo."],"metadata":{"id":"BHv9HlbXhiSG"}},{"cell_type":"code","source":["# Importar librerías\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"dy6uwAS7ij21","executionInfo":{"status":"ok","timestamp":1719528083165,"user_tz":300,"elapsed":1065,"user":{"displayName":"Talento Tech","userId":"17477515765803143194"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Cargar y preparar el dataset"],"metadata":{"id":"AyGx24fzi7zn"}},{"cell_type":"code","source":["# Cargar el dataset de reseñas de IMDb\n","\n","# Se divide en conjunto de entrenamiento en 90% y 10% validación\n","raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n","    name=\"imdb_reviews\",\n","    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n","    as_supervised=True  # Cargar datos como pares (texto, etiqueta)\n",")\n","\n","# Establecer una semilla para reproducibilidad\n","tf.random.set_seed(42)\n","\n","# Preparar el conjunto de entrenamiento\n","# 1. Mezclar (shuffle) el conjunto con un buffer de 5000 y semilla para reproducibilidad\n","# 2. Agrupar (batch) los datos en lotes de tamaño 32\n","# 3. Prefetch para mejorar el rendimiento cargando los datos en segundo plano\n","train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n","\n","# Preparar el conjunto de validación\n","# 1. Agrupar (batch) los datos en lotes de tamaño 32\n","# 2. Prefetch para mejorar el rendimiento cargando los datos en segundo plano\n","valid_set = raw_valid_set.batch(32).prefetch(1)\n","\n","# Preparar el conjunto de prueba\n","# 1. Agrupar (batch) los datos en lotes de tamaño 32\n","# 2. Prefetch para mejorar el rendimiento cargando los datos en segundo plano\n","test_set = raw_test_set.batch(32).prefetch(1)"],"metadata":{"id":"ZqFvsf_VitPW","executionInfo":{"status":"ok","timestamp":1719528085907,"user_tz":300,"elapsed":2744,"user":{"displayName":"Talento Tech","userId":"17477515765803143194"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Mostrar ejemplos de reseñas y sus etiquetas del conjunto de entrenamiento"],"metadata":{"id":"ZcfDtbvAjep-"}},{"cell_type":"code","source":["# Iterar sobre los primeros 4 ejemplos del conjunto de entrenamiento\n","for review, label in raw_train_set.take(4):\n","    # Imprimir los primeros 200 caracteres de la reseña\n","    print(review.numpy().decode(\"utf-8\")[:200], \"...\")\n","    # Imprimir la etiqueta asociada a la reseña\n","    print(\"Label:\", label.numpy())"],"metadata":{"id":"SS6yT5nbjc_A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719528085909,"user_tz":300,"elapsed":6,"user":{"displayName":"Talento Tech","userId":"17477515765803143194"}},"outputId":"ce9f4ff0-a82f-418f-b199-4027b923f78a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n","Label: 0\n","I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n","Label: 0\n","Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Moun ...\n","Label: 0\n","This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful perf ...\n","Label: 1\n"]}]},{"cell_type":"markdown","source":["Configurar y adaptar la capa de vectorización de texto.\n","\n","Limitaremos el vocabulario a 1.000 tokens, incluyendo las 998 palabras más frecuentes más un token de padding y un token para palabras desconocidas."],"metadata":{"id":"I8YsY3SZju5f"}},{"cell_type":"code","source":["# Definir el tamaño del vocabulario\n","vocab_size = 1000\n","\n","# Crear una capa de vectorización de texto con un tamaño máximo de vocabulario de 1000 tokens\n","text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n","\n","# Adaptar la capa de vectorización al texto del conjunto de entrenamiento\n","# La capa aprenderá el vocabulario a partir de los datos de entrenamiento\n","text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))"],"metadata":{"id":"tB4Vt5HKjvBI","executionInfo":{"status":"ok","timestamp":1719528091474,"user_tz":300,"elapsed":5569,"user":{"displayName":"Talento Tech","userId":"17477515765803143194"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["`text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)`:\n","\n","Crea una capa de vectorización de texto de Keras que convierte el texto en secuencias de enteros, limitando el vocabulario a `vocab_size` palabras más frecuentes."],"metadata":{"id":"bcsnHAHCkRz_"}},{"cell_type":"markdown","source":["## Definir y entrenar un modelo de clasificación de texto con una capa de Embedding y una capa LSTM"],"metadata":{"id":"PxWGBW-Pkuzu"}},{"cell_type":"code","source":["# Esta celda toma varios minutos si no se ejecuta en GPU\n","\n","# Definir el tamaño de los embeddings, que determina la dimensión de los vectores de representación de las palabras\n","embed_size = 128\n","\n","# Establecer una semilla para reproducibilidad\n","tf.random.set_seed(42)\n","\n","# Definir el modelo secuencial de Keras\n","model = tf.keras.Sequential([\n","    text_vec_layer,  # Capa de vectorización de texto para convertir texto en secuencias de enteros\n","    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),  # Capa de embedding con tamaño de vocabulario y tamaño de embeddings\n","    tf.keras.layers.LSTM(128),  # Capa LSTM con 128 unidades\n","    tf.keras.layers.Dense(1, activation=\"sigmoid\")  # Capa densa con activación sigmoide para clasificación binaria\n","])\n","\n","# Compilar el modelo con una función de pérdida, un optimizador y métricas\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n","\n","# Entrenar el modelo con el conjunto de entrenamiento y validación durante 5 épocas\n","history = model.fit(train_set, validation_data=valid_set, epochs=5)"],"metadata":{"id":"-FiHqVw3kalT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719528298811,"user_tz":300,"elapsed":207339,"user":{"displayName":"Talento Tech","userId":"17477515765803143194"}},"outputId":"cce985bf-8c25-458b-8636-20905eb72914"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","704/704 [==============================] - 56s 70ms/step - loss: 0.5285 - accuracy: 0.7380 - val_loss: 0.4777 - val_accuracy: 0.7836\n","Epoch 2/5\n","704/704 [==============================] - 28s 39ms/step - loss: 0.4250 - accuracy: 0.8158 - val_loss: 0.4480 - val_accuracy: 0.8088\n","Epoch 3/5\n","704/704 [==============================] - 27s 38ms/step - loss: 0.3730 - accuracy: 0.8425 - val_loss: 0.3821 - val_accuracy: 0.8480\n","Epoch 4/5\n","704/704 [==============================] - 26s 37ms/step - loss: 0.4097 - accuracy: 0.8192 - val_loss: 0.3962 - val_accuracy: 0.8276\n","Epoch 5/5\n","704/704 [==============================] - 27s 38ms/step - loss: 0.3463 - accuracy: 0.8537 - val_loss: 0.5225 - val_accuracy: 0.7300\n"]}]},{"cell_type":"markdown","source":["`tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)`\n","\n","`vocab_size`: Tamaño del vocabulario, es decir, el número total de tokens únicos que la capa de embedding puede manejar.\n","\n","`embed_size`: Dimensión de los vectores de embedding, es decir, el tamaño del vector que representa cada token.\n","\n","`mask_zero=True`: Indica que los tokens con valor cero (generalmente usados para padding) deben ser ignorados en el cálculo de los embeddings y en el posterior procesamiento."],"metadata":{"id":"CU8k--9Ol9an"}},{"cell_type":"code","source":[],"metadata":{"id":"3bWUAGYMowCo","executionInfo":{"status":"ok","timestamp":1719528298811,"user_tz":300,"elapsed":5,"user":{"displayName":"Talento Tech","userId":"17477515765803143194"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Análisis de Sentimiento con Redes pre-entrenadas\n","\n","Existen varias redes de modelos de lenguaje pre-entranadas disponibles, como:\n","\n","[Word2Vec](https://www.tensorflow.org/text/tutorials/word2vec) de Google\n","\n","[GloVe embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/) de Stanford\n","\n","[FastText embeddings](https://fasttext.cc/) de Facebook\n","\n","En este caso vamos a usar el [Universal Sentence Encoder](https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder) de Google"],"metadata":{"id":"dHV3guL1owaZ"}},{"cell_type":"code","source":["# Esta celda tomará varios minutos, incluso en GPU\n","import os\n","import tensorflow_hub as hub\n","\n","# Establecer la ruta del directorio de caché de TF Hub\n","os.environ[\"TFHUB_CACHE_DIR\"] = \"my_tfhub_cache\"\n","tf.random.set_seed(42)\n","\n","# Definir el modelo secuencial de Keras\n","model = tf.keras.Sequential([\n","    # Capa de TF Hub que utiliza el Universal Sentence Encoder (USE)\n","    hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n","                   trainable=True, dtype=tf.string, input_shape=[]),\n","    # Capa densa con 64 unidades y activación ReLU\n","    tf.keras.layers.Dense(64, activation=\"relu\"),\n","    # Capa de salida densa con activación sigmoide para clasificación binaria\n","    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n","              metrics=[\"accuracy\"])\n","model.fit(train_set, validation_data=valid_set, epochs=10)"],"metadata":{"id":"QPtuSGB9ms-A","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2d48177a-6e77-4df8-cbc4-c207788acc06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","704/704 [==============================] - 2605s 4s/step - loss: 0.2934 - accuracy: 0.8761 - val_loss: 0.2397 - val_accuracy: 0.9016\n","Epoch 2/10\n"," 77/704 [==>...........................] - ETA: 1:30:05 - loss: 0.0492 - accuracy: 0.9825"]}]},{"cell_type":"markdown","source":["## Probar el mejor modelo en el conjunto de prueba"],"metadata":{"id":"5aLuZHL2y6AA"}},{"cell_type":"code","source":["model.evaluate(test_set)"],"metadata":{"id":"g-DUUQI4y5fG"},"execution_count":null,"outputs":[]}]}