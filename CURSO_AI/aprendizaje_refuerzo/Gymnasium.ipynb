{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introducción a Gymnasium\n","En este notebook usaremos [gymnasium](https://gymnasium.farama.org/index.html), un gran conjunto de herramientas para desarrollar y comparar algoritmos de Aprendizaje por Refuerzo. Proporciona muchos entornos para que tus agentes de aprendizaje interactúen."],"metadata":{"id":"5AP1PN1irsEP"}},{"cell_type":"code","source":["# Este comando instala las versiones más recientes de las bibliotecas `gymnasium` y `swig`.\n","# La opción `-q` se utiliza para que la salida de la instalación muestre menos información (quiet).\n","# La opción `-U` se utiliza para asegurar que se actualicen a la última versión disponible (upgrade).\n","%pip install -q -U gymnasium swig\n","\n","# Este comando instala `gymnasium` con algunos conjuntos de entornos específicos.\n","# La opción `classic_control` incluye entornos clásicos de control como CartPole y MountainCar.\n","# La opción `box2d` incluye entornos que utilizan la biblioteca Box2D, como BipedalWalker y LunarLander.\n","# La opción `atari` incluye entornos de juegos de Atari, que son populares para pruebas de algoritmos de RL.\n","# La opción `accept-rom-license` acepta automáticamente la licencia de las ROMs de Atari, necesaria para usar estos entornos.\n","%pip install -q -U gymnasium[classic_control,box2d,atari,accept-rom-license]"],"metadata":{"id":"A3rM0C-eqQBT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos a crear un entorno CartPole versión 1. Esta es una simulación 2D de un entorno muy sencillo compuesto por un carrito que puede moverse a izquierda o derecha, y un palo colocado verticalmente encima. El agente debe mover el carrito a izquierda o derecha para mantener el palo en posición vertical.\n","Esta es una clásica tarea de control."],"metadata":{"id":"ZqLECNPpsjem"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRkcRS69qDLN"},"outputs":[],"source":["# Importa la biblioteca gymnasium y la asigna al alias 'gym' para facilitar su uso.\n","import gymnasium as gym\n","\n","# Crea un entorno para el problema de CartPole (versión 1) utilizando la función make() de gym.\n","# El parámetro render_mode=\"rgb_array\" especifica que el entorno debe renderizarse como una matriz de valores RGB.\n","# Esto es útil para obtener representaciones visuales del entorno sin necesidad de una interfaz gráfica en tiempo real.\n","env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"]},{"cell_type":"code","source":["# Especificaciones para el entorno CartPole-v1\n","envs = gym.envs.registry\n","envs[\"CartPole-v1\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aZM9vbKRszTI","executionInfo":{"status":"ok","timestamp":1720215923982,"user_tz":300,"elapsed":11,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"e06451b7-d8e4-4455-af96-00847bfd5b50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"execute_result","data":{"text/plain":["EnvSpec(id='CartPole-v1', entry_point='gymnasium.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=1, additional_wrappers=(), vector_entry_point='gymnasium.envs.classic_control.cartpole:CartPoleVectorEnv')"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["Una vez creado el entorno, debe inicializarlo utilizando el método` reset()`, especificando opcionalmente una semilla aleatoria. Esto devuelve la primera observación.\n","\n","Las observaciones dependen del tipo de entorno. Para el entorno CartPole, cada observación es una matriz NumPy 1D que contiene cuatro valores float que representan:\n","* Posición horizontal del carro (0.0 = centro).\n","* Velocidad (positivo significa derecha).\n","* Ángulo del palo (0.0 = vertical)\n","* Velocidad angular (positivo significa en el sentido de las agujas del reloj).\n","\n","El método `reset()` también devuelve un diccionario que puede contener información extra específica del entorno. Esto puede ser útil para la depuración o para la formación. Por ejemplo, en muchos entornos Atari, contiene el número de vidas restantes. Sin embargo, en el entorno CartPole, este diccionario está vacío."],"metadata":{"id":"MJaU2xLjtvCz"}},{"cell_type":"code","source":["obs, info = env.reset(seed=42)\n","obs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3SphgwPtcds","executionInfo":{"status":"ok","timestamp":1720215923982,"user_tz":300,"elapsed":8,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"f2a5feb3-7355-474b-d956-cc824f770969"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["Llamemos al método `render()` para renderizar este entorno como una imagen. Dado que establecimos `render_mode=\"rgb_array\"` al crear el entorno, la imagen será devuelta como un array NumPy:"],"metadata":{"id":"MMz2Bb30uSLa"}},{"cell_type":"code","source":["img = env.render()\n","img.shape # alto, ancho, canales (3 = RGB)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yw7Zk5o2uKQh","executionInfo":{"status":"ok","timestamp":1720215923982,"user_tz":300,"elapsed":6,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"7ab1c41e-8231-4074-8676-361cdf240009"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(400, 600, 3)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","def plot_environment(env, figsize=(5, 4)):\n","    plt.figure(figsize=figsize)\n","\n","    # Renderiza el entorno 'env' y guarda la imagen resultante en 'img'.\n","    img = env.render()\n","\n","    # Muestra la imagen 'img' en la figura actual.\n","    plt.imshow(img)\n","\n","    plt.axis(\"off\")\n","    return img\n","\n","plot_environment(env)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"vs3V9l7-u95f","executionInfo":{"status":"ok","timestamp":1720215924386,"user_tz":300,"elapsed":409,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"cee9bf61-f337-43fc-8e4c-8105d3f10c64"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 500x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["Veamos cómo interactuar con el entorno. Tu agente tendrá que seleccionar una acción de un **\"espacio de acción\"** (el conjunto de acciones posibles). Veamos cómo es el espacio de acción de este entorno:"],"metadata":{"id":"7rEoRuANv6tr"}},{"cell_type":"code","source":["env.action_space"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buWWSTC6vPeg","executionInfo":{"status":"ok","timestamp":1720215924386,"user_tz":300,"elapsed":27,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"fe3cfa3c-a561-491e-ba42-f5ff2cf85986"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Discrete(2)"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["Sí, sólo dos acciones posibles:\n","* Acelerar hacia la izquierda (0)\n","* Acelerar hacia la derecha (1).\n","\n","Como el palo se inclina hacia la derecha (`obs[2] > 0`), aceleremos el carro hacia la derecha:"],"metadata":{"id":"7AEfbq1jwDm2"}},{"cell_type":"code","source":["action = 1  # Definición de la acción: acelerar hacia la derecha\n","\n","# Ejecutar la acción en el entorno (con la función step) y obtener el siguiente estado\n","obs, reward, done, truncated, info = env.step(action)\n","\n","# Devolver el estado observado después de ejecutar la acción\n","obs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AHntv4irv_-B","executionInfo":{"status":"ok","timestamp":1720215924386,"user_tz":300,"elapsed":24,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"924bbc02-d339-4b7f-9048-28be4bae8411"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.02727336,  0.18847767,  0.03625453, -0.26141977], dtype=float32)"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["Recordemos: obs=[Posición horizontal del carro (0.0 = centro), Velocidad (positivo significa derecha), Ángulo del palo (0.0 = vertical), Velocidad angular (positivo significa en el sentido de las agujas del reloj)]"],"metadata":{"id":"KIuxIp35xDpa"}},{"cell_type":"markdown","source":["- `obs` contiene el estado observado después de ejecutar la acción. El carro se mueve ahora hacia la derecha (`obs[1] > 0`). El palo sigue inclinado hacia la derecha (`obs[2] > 0`), pero su velocidad angular es ahora negativa (`obs[3] < 0`), por lo que probablemente se inclinará hacia la izquierda tras el siguiente paso.\n","- `reward` es la recompensa obtenida por realizar la acción.\n","En este entorno, obtienes una recompensa de 1.0 a cada paso,\n","hagas lo que hagas, mientras el palo no se caiga, así que el objetivo es mantener el episodio en marcha el mayor tiempo posible.\n","- `done` indica si el episodio ha terminado (True o False).\n","- `truncated` indica si el episodio se ha truncado prematuramente (True o False).\n","- `info` contiene información adicional del entorno (opcional, depende del entorno)."],"metadata":{"id":"qs18sgKJx6RE"}},{"cell_type":"code","source":["plot_environment(env)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"J-_1dYoXwbfs","executionInfo":{"status":"ok","timestamp":1720215924641,"user_tz":300,"elapsed":278,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"da6a4cb4-200c-4e8d-e4a1-508f218bb795"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 500x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["El entorno también indica al agente cuánta recompensa obtuvo durante el último paso:"],"metadata":{"id":"9oRoGK-hyy5d"}},{"cell_type":"code","source":["reward"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aRLuVWUKyzUD","executionInfo":{"status":"ok","timestamp":1720215924641,"user_tz":300,"elapsed":16,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"02a10f95-1001-44a7-d32c-643d220ed812"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["Cuando el juego termina, el entorno devuelve done=True. En este caso, aún no ha terminado:"],"metadata":{"id":"-mMNDSwLy3Ny"}},{"cell_type":"code","source":["done"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9TeS468y0W4","executionInfo":{"status":"ok","timestamp":1720215924642,"user_tz":300,"elapsed":14,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"a4bcda82-4f8d-46c1-e24d-9eba31e3ac17"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["Algunas herramientas de entorno pueden querer interrumpir el entorno antes de tiempo. Por ejemplo, cuando se alcanza un límite de tiempo o cuando un objeto sale de los límites. En este caso, truncado se pondrá a True. En este caso, aún no está truncado:"],"metadata":{"id":"vxNEEeb3zB7z"}},{"cell_type":"code","source":["truncated"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tdTORN6ty5vI","executionInfo":{"status":"ok","timestamp":1720215924642,"user_tz":300,"elapsed":13,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"76019b9f-c5a2-45e9-e516-cedb5f68a692"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["Por último, `info` es un diccionario específico del entorno que puede proporcionar alguna información extra que te puede resultar útil para depurar o para entrenar. Por ejemplo, en algunos juegos puede indicar cuántas vidas tiene el agente."],"metadata":{"id":"wRCCb9FgzJGr"}},{"cell_type":"code","source":["info"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_IAzmETGzKkC","executionInfo":{"status":"ok","timestamp":1720215924642,"user_tz":300,"elapsed":11,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"a6602c4a-6c53-405b-97ac-8fdeacc04564"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{}"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["La secuencia de pasos entre el momento en que se inicia el entorno (`reset`) hasta que se termina o se trunca se denomina **\"episodio\"**. Al final de un episodio (es decir, cuando `step()` devuelve `done=True` o `truncated=True`), debes reiniciar el entorno antes de seguir utilizándolo."],"metadata":{"id":"3KAJnMg3yq3b"}},{"cell_type":"code","source":["if done or truncated:\n","    obs, info = env.reset()"],"metadata":{"id":"V5oy8QWlyjQ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Una simple política de código en reglas.\n","Ahora, ¿cómo podemos hacer que el palo no se caiga?\n","\n","Para ello tendremos que definir una **política**. Esta es la estrategia que el agente utilizará para seleccionar una acción en cada paso. Puede utilizar todas las acciones y observaciones pasadas para decidir qué hacer.\n","\n","Vamos a codificar una estrategia simple: si el palo se inclina hacia la izquierda, entonces empujemos el carro hacia la izquierda, y viceversa."],"metadata":{"id":"t-TomWuq0Mhu"}},{"cell_type":"code","source":["# Definición de una política básica que decide la acción basada en el ángulo del palo.\n","def basic_policy(obs):\n","    angle = obs[2]  # Se obtiene el ángulo del palo del estado observado 'obs'.\n","    return 0 if angle < 0 else 1  # Devuelve 0 (izquierda) si el ángulo es negativo, o 1 (derecha) si es positivo o cero.\n","\n","totals = []  # Lista para almacenar las recompensas totales de cada episodio.\n","\n","# Ciclo principal para ejecutar 500 episodios de entrenamiento.\n","for episode in range(500):\n","    episode_rewards = 0  # Inicialización de la recompensa acumulada para este episodio.\n","\n","    # Reiniciar el entorno para comenzar un nuevo episodio, utilizando una semilla diferente para cada episodio.\n","    obs, info = env.reset(seed=episode)\n","\n","    # Ciclo para ejecutar pasos dentro del episodio (hasta un máximo de 200 pasos).\n","    for step in range(200):\n","        action = basic_policy(obs)  # Determinar la acción utilizando la política básica.\n","        obs, reward, done, truncated, info = env.step(action)  # Ejecutar la acción y obtener la respuesta del entorno.\n","\n","        episode_rewards += reward  # Acumular la recompensa obtenida en este paso al total del episodio.\n","\n","        # Salir del bucle si el episodio ha terminado (por ejemplo, el palo ha caído) o si se ha truncado.\n","        if done or truncated:\n","            break\n","\n","    # Almacenar la recompensa total del episodio en la lista 'totals'.\n","    totals.append(episode_rewards)"],"metadata":{"id":"CWkVAOdFz4p4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Veamos los resultados\n","import numpy as np\n","\n","np.mean(totals), np.std(totals), min(totals), max(totals)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oTV-n2XV1khL","executionInfo":{"status":"ok","timestamp":1720215925271,"user_tz":300,"elapsed":8,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"49f45bc0-273f-4227-916c-4f480d74b98e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(41.698, 8.389445512070509, 24.0, 63.0)"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["Bueno, como era de esperar, esta estrategia es demasiado básica: lo mejor que hizo fue mantener el palo durante sólo 63 pasos. Este entorno se considera resuelto cuando el agente mantiene el palo durante 200 pasos."],"metadata":{"id":"WdEM_2TD1xJs"}},{"cell_type":"markdown","source":["# *Policies* con redes neuronales\n","Veamos si una red neuronal puede idear una política mejor.\n","\n","La red neuronal que tomará las observaciones como entradas y emitirá las probabilidades de las acciones a tomar para cada observación.\n","\n","Para elegir una acción, la red estimará una probabilidad para cada acción y, a continuación, seleccionaremos una acción al azar de acuerdo con las probabilidades estimadas.\n","\n","En el caso del entorno CartPole, sólo hay dos acciones posibles (izquierda o derecha), por lo que sólo necesitamos una neurona de salida: emitirá la probabilidad *p* de la acción 0 (izquierda) y, por supuesto, la probabilidad de la acción 1 (derecha) será *1 - p*."],"metadata":{"id":"WSqXc5rp2J5K"}},{"cell_type":"code","source":["# Implementemos una red muy sencilla con una sola capa oculta con 5 neuronas\n","\n","import tensorflow as tf\n","\n","tf.random.set_seed(42)\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(5, activation=\"relu\"),\n","    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n","])"],"metadata":{"id":"4tFTVBxD1rTx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Utilizamos un modelo secuencial para definir la  **política**. El número de entradas es el tamaño del espacio de observación -que en el caso de CartPole es 4- y tenemos sólo cinco neuronas ocultas porque es una tarea bastante simple.\n","\n","Por último, queremos dar salida a una única probabilidad -la probabilidad de ir a la izquierda-, así que tenemos una única neurona de salida que utiliza la función de activación sigmoide. Si hubiera más de dos acciones posibles, habría una neurona de salida por acción y utilizaríamos la función de activación Softmax.\n","\n","Bien, ya tenemos una política de red neuronal que tomará observaciones y emitirá probabilidades de acción. Pero, ¿cómo la entrenamos?"],"metadata":{"id":"l19z__vd7y4u"}},{"cell_type":"markdown","source":["Vamos a utilizar Keras para implementar este algoritmo.\n","\n","Entrenaremos la política de la red neuronal que construimos anteriormente para que aprenda a equilibrar el palo en el carro.\n","\n","Primero, necesitamos una función que ejecute un paso. Vamos a pretender por ahora que cualquier acción que toma es la correcta para que podamos calcular la pérdida y sus gradientes.\n","\n","Estos gradientes se guardarán por un tiempo, y los modificaremos más tarde dependiendo de lo buena o mala que haya resultado la acción:"],"metadata":{"id":"8Etx8Bq4E-T7"}},{"cell_type":"code","source":["def play_one_step(env, obs, model, loss_fn):\n","\n","    # tf.GradientTape() Registre el cálculo automático de gradientes.\n","    with tf.GradientTape() as tape:\n","\n","        # Obtiene la probabilidad de mover hacia la izquierda dada la observación actual.\n","        left_proba = model(obs[np.newaxis])\n","\n","        # Determina la acción basándose en una comparación aleatoria con la probabilidad de mover hacia la izquierda.\n","        action = (tf.random.uniform([1, 1]) > left_proba)\n","\n","        # Define el objetivo \"y\" (1 si la acción es moverse a la derecha, 0 si es a la izquierda).\n","        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n","\n","        # Calcula la pérdida utilizando la función de pérdida proporcionada.\n","        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n","\n","    # Calcula los gradientes de la pérdida con respecto a las variables entrenables del modelo.\n","    grads = tape.gradient(loss, model.trainable_variables)\n","\n","    # Ejecuta la acción en el entorno y obtiene la nueva observación, recompensa y estado del episodio.\n","    obs, reward, done, truncated, info = env.step(int(action))\n","\n","    # Devuelve la nueva observación, la recompensa, el estado del episodio y los gradientes calculados.\n","    return obs, reward, done, truncated, grads"],"metadata":{"id":"w2agaKitFEk7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Dentro del bloque `GradientTape` , empezamos por llamar al modelo, dándole una sola observación. Reformamos la observación para que se convierta en un lote que contenga una única instancia, ya que el modelo espera un lote. Así se obtiene la probabilidad de ir a la izquierda.\n","- A continuación, muestreamos una variable aleatoria entre 0 y 1, y comprobamos si es mayor que  `left_proba`. La acción será `False` con probabilidad `left_proba`, o `True` con probabilidad `1 - left_proba`. Una vez convertido este booleano en un entero, la acción será 0 (izquierda) o 1 (derecha) con las probabilidades apropiadas.\n","- Ahora definimos la probabilidad objetivo de ir a la izquierda: es 1 menos la acción (convertida en flotante). Si la acción es 0 (izquierda), la probabilidad objetivo de ir a la izquierda será 1. Si la acción es 1 (derecha), entonces la probabilidad objetivo será 0.\n","- A continuación, calculamos la pérdida utilizando la función de pérdida dada, y utilizamos `tape.gradient` para calcular el gradiente de la pérdida con respecto a las variables entrenables del modelo. De nuevo, estos gradientes se ajustarán más tarde, antes de aplicarlos, dependiendo de lo buena o mala que haya resultado la acción.\n","- Por último, ejecutamos la acción seleccionada y devolvemos la nueva observación, la recompensa, si el episodio ha terminado o no, si se ha truncado o no y, por supuesto, los gradientes que acabamos de calcular."],"metadata":{"id":"aI_SQNfkFU0B"}},{"cell_type":"markdown","source":["Ahora vamos a crear otra función que se basará en la función `play_one_step()` para reproducir múltiples episodios, devolviendo todas las recompensas y gradientes para cada episodio y cada paso:"],"metadata":{"id":"2zIw9CIZGcEn"}},{"cell_type":"code","source":["def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n","    all_rewards = []  # Lista para almacenar las recompensas de todos los episodios.\n","    all_grads = []    # Lista para almacenar los gradientes de todos los episodios.\n","\n","    for episode in range(n_episodes):\n","        current_rewards = []  # Lista para almacenar las recompensas del episodio actual.\n","        current_grads = []    # Lista para almacenar los gradientes del episodio actual.\n","\n","        # Reinicia el entorno al comienzo de cada episodio.\n","        obs, info = env.reset()\n","\n","        for step in range(n_max_steps):\n","            # Juega un paso en el entorno utilizando el modelo y la función de pérdida.\n","            obs, reward, done, truncated, grads = play_one_step(env, obs, model, loss_fn)\n","\n","            # Almacena la recompensa y los gradientes del paso actual.\n","            current_rewards.append(reward)\n","            current_grads.append(grads)\n","\n","            # Termina el episodio si el entorno indica que ha terminado o ha sido truncado.\n","            if done or truncated:\n","                break\n","\n","        # Almacena las recompensas y los gradientes del episodio actual.\n","        all_rewards.append(current_rewards)\n","        all_grads.append(current_grads)\n","\n","    # Devuelve las recompensas y los gradientes de todos los episodios.\n","    return all_rewards, all_grads\n"],"metadata":{"id":"FB3-RzO4FUUD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Este código devuelve una lista de listas de recompensas: una lista de recompensas por episodio, que contiene una recompensa por paso. También devuelve una lista de listas de gradientes: una lista de gradientes por episodio, cada una conteniendo una tupla de gradientes por paso y cada tupla conteniendo un tensor de gradiente por variable entrenable.\n","\n","El algoritmo utilizará la función `play_multiple_episodes()` para jugar el juego varias veces (por ejemplo, 10 veces), luego volverá atrás y mirará todas las recompensas, las descontará y las normalizará. Para ello, necesitamos un par de funciones más; la primera calculará la suma de las futuras recompensas descontadas en cada paso, y la segunda normalizará todas estas recompensas descontadas (es decir, los rendimientos) a través de muchos episodios restando la media y dividiendo por la desviación estándar:"],"metadata":{"id":"k5yL2i-hGoEm"}},{"cell_type":"code","source":["def discount_rewards(rewards, discount_factor):\n","    # Convierte la lista de recompensas en un array numpy.\n","    discounted = np.array(rewards)\n","\n","    # Recorre las recompensas desde el penúltimo elemento hasta el primero.\n","    for step in range(len(rewards) - 2, -1, -1):\n","        # Aplica el factor de descuento acumulativo.\n","        discounted[step] += discounted[step + 1] * discount_factor\n","\n","    # Devuelve las recompensas descontadas.\n","    return discounted\n","\n","def discount_and_normalize_rewards(all_rewards, discount_factor):\n","    # Calcula las recompensas descontadas para cada episodio.\n","    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n","                              for rewards in all_rewards]\n","\n","    # Aplana (flatten) la lista de recompensas descontadas para calcular la media y la desviación estándar.\n","    flat_rewards = np.concatenate(all_discounted_rewards)\n","    reward_mean = flat_rewards.mean()\n","    reward_std = flat_rewards.std()\n","\n","    # Normaliza las recompensas descontadas restando la media y dividiendo por la desviación estándar.\n","    return [(discounted_rewards - reward_mean) / reward_std\n","            for discounted_rewards in all_discounted_rewards]\n","\n","# Retorna una lista donde cada elemento contiene las recompensas descontadas y normalizadas de cada episodio"],"metadata":{"id":"hVljur_JGinz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Veamos si esto funciona.\n","\n","Digamos que hubo 3 acciones, y que después de cada acción hubo una recompensa:\n","\n","Primero 10, luego 0, luego -50.\n","\n","Si utilizamos un factor de descuento del 80%, entonces:\n","- La 3ª acción obtendrá -50 (crédito completo por la última recompensa)\n","- La 2ª acción sólo obtendrá -40 (crédito del 80% por la última recompensa)\n","- La 1ª acción obtendrá el 80% de -40 (-32) más el crédito completo por la primera recompensa (+10), lo que lleva a una recompensa descontada de -22:"],"metadata":{"id":"qzbztJuiHNvF"}},{"cell_type":"code","source":["discount_rewards([10, 0, -50], discount_factor=0.8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VS733JsTHIrL","executionInfo":{"status":"ok","timestamp":1720220292563,"user_tz":300,"elapsed":271,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"ca41973b-dc5c-4216-ffd7-c001518dd02a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-22, -40, -50])"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["Para normalizar todas las recompensas descontadas en todos los episodios, calculamos la media y la desviación estándar de todas las recompensas descontadas, restamos la media de cada recompensa descontada y dividimos por la desviación estándar:"],"metadata":{"id":"qQgjZ5s7H6Sv"}},{"cell_type":"code","source":["discount_and_normalize_rewards([[10, 0, -50], [10, 20]],\n","                               discount_factor=0.8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zDIbvfSfHpNb","executionInfo":{"status":"ok","timestamp":1720220375377,"user_tz":300,"elapsed":284,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"e5e1435b-4dd6-4a5b-b9f7-9c86feabb14a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([-0.28435071, -0.86597718, -1.18910299]),\n"," array([1.26665318, 1.0727777 ])]"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["Ya estamos casi listos para ejecutar el algoritmo. Ahora vamos a definir los hiperparámetros. Ejecutaremos 150 iteraciones de entrenamiento, reproduciendo 10 episodios por iteración, y cada episodio durará como máximo 200 pasos. Utilizaremos un factor de descuento de 0.95:"],"metadata":{"id":"Wzq8XRB0IHCE"}},{"cell_type":"code","source":["n_iterations = 150\n","n_episodes_per_update = 10\n","n_max_steps = 200\n","discount_factor = 0.95\n","\n","obs, info = env.reset(seed=42)"],"metadata":{"id":"QlWQ0mZDH9b9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["También necesitamos un optimizador y una función de pérdida. Un optimizador Nadam normal con una tasa de aprendizaje de 0,01 funcionará bien, y utilizaremos la función de pérdida de entropía cruzada binaria porque estamos entrenando un clasificador binario (hay dos acciones posibles: izquierda o derecha):"],"metadata":{"id":"Gxa4Y7siIYjD"}},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n","loss_fn = tf.keras.losses.binary_crossentropy"],"metadata":{"id":"7wmjQ_2TIZ89"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ya estamos listos para construir y ejecutar el bucle de entrenamiento"],"metadata":{"id":"LFUujtWDIess"}},{"cell_type":"code","source":["for iteration in range(n_iterations):\n","    # Juega múltiples episodios y obtiene las recompensas y gradientes de cada uno.\n","    all_rewards, all_grads = play_multiple_episodes(\n","        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n","\n","    # Muestra información de depuración durante el entrenamiento.\n","    total_rewards = sum(map(sum, all_rewards))\n","    print(f\"\\rIteration: {iteration + 1}/{n_iterations},\"\n","          f\" mean rewards: {total_rewards / n_episodes_per_update:.1f}\", end=\"\")\n","\n","    # Descuenta y normaliza las recompensas de todos los episodios.\n","    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n","\n","    all_mean_grads = []  # Lista para almacenar los gradientes medios de todas las variables del modelo.\n","\n","    # Calcula los gradientes medios para cada variable entrenable del modelo.\n","    # Este bucle itera a través de todos los índices de las variables entrenables del modelo\n","    # Cada var_index corresponde a una variable entrenable específica del modelo, como un peso o un sesgo.\n","    for var_index in range(len(model.trainable_variables)):\n","        # tf.reduce_mean calcula la media de los elementos a través de las dimensiones de un tensor.\n","        mean_grads = tf.reduce_mean(\n","            [final_reward * all_grads[episode_index][step][var_index]\n","             for episode_index, final_rewards in enumerate(all_final_rewards)\n","                 for step, final_reward in enumerate(final_rewards)], axis=0)\n","        all_mean_grads.append(mean_grads)\n","\n","    # Aplica los gradientes medios a las variables entrenables del modelo utilizando el optimizador.\n","    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6INL2uQyIeiu","executionInfo":{"status":"ok","timestamp":1720221533101,"user_tz":300,"elapsed":1011441,"user":{"displayName":"Gabriel Narváez","userId":"00215126040562476601"}},"outputId":"d89ee146-2895-439d-bf78-c35bdd631551"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py:291: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n","  return int(self._numpy())\n"]},{"output_type":"stream","name":"stdout","text":["Iteration: 150/150, mean rewards: 188.8"]}]},{"cell_type":"markdown","source":["Analicemos este código:\n","* En cada iteración de entrenamiento, este bucle llama a la función `play_multiple_episodes()`, que reproduce 10 episodios y devuelve las recompensas y los gradientes de cada paso en cada episodio.\n","* A continuación, llamamos a la función `discount_and_normalize_rewards()` para calcular la ventaja normalizada de cada acción, denominada `final_reward` en este código. Esto proporciona una medida de lo buena o mala que fue cada acción en retrospectiva.\n","* A continuación, pasamos por cada variable entrenable, y para cada una de ellas calculamos la media ponderada de los gradientes para esa variable sobre todos los episodios y todos los pasos, ponderados por `final_reward`.\n","* Por último, aplicamos estos gradientes medios mediante el optimizador: las variables entrenables del modelo se ajustarán y, con suerte, la política será un poco mejor."],"metadata":{"id":"Pko0gpkAzh-G"}},{"cell_type":"code","source":[],"metadata":{"id":"qhW1x3yzIbHa"},"execution_count":null,"outputs":[]}]}